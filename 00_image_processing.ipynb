{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_base_path = './'\n",
    "\n",
    "fake_dataset_dir = f\"{relative_base_path}dataset/manipulated_sequences\"\n",
    "real_dataset_dir = f\"{relative_base_path}dataset/original_sequences\"\n",
    "\n",
    "fake_output_dir = f\"{relative_base_path}out/fake\"\n",
    "real_output_dir = f\"{relative_base_path}out/real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10981,
     "status": "ok",
     "timestamp": 1735395270035,
     "user": {
      "displayName": "Faraz Khalil",
      "userId": "15571848968372978323"
     },
     "user_tz": 0
    },
    "id": "aTQQcaPUGJqS",
    "outputId": "358f5169-68d0-4fdc-ccf6-fb3640a5cd3b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from models.blink_detection.DetectBlinking import DetectBlinking\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(features, n_components=50):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "\n",
    "    return reduced_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1735395277456,
     "user": {
      "displayName": "Faraz Khalil",
      "userId": "15571848968372978323"
     },
     "user_tz": 0
    },
    "id": "Wyuyb2jUHYPN"
   },
   "outputs": [],
   "source": [
    "def save(path, output_filename, features):\n",
    "    if not (os.path.exists(f\"{path}{output_filename}\")):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        ds = {\"ORE_MAX_GIORNATA\": 5}\n",
    "        np.savez_compressed(os.path.join(path, output_filename), ds)\n",
    "\n",
    "    print(f\"Video Processed | Features: \", features.shape)\n",
    "    np.savez_compressed(f\"{path}/{output_filename}\", features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1735395464855,
     "user": {
      "displayName": "Faraz Khalil",
      "userId": "15571848968372978323"
     },
     "user_tz": 0
    },
    "id": "trgdPoukHcRw"
   },
   "outputs": [],
   "source": [
    "def extract_features_and_save(video_paths, output_dir, output_files):\n",
    "    saved_useless_files = np.load(\"out/useless_files.npy\")\n",
    "\n",
    "    useless_files = saved_useless_files.tolist()\n",
    "    print(\"saved_useless_files: \", len(useless_files))\n",
    "    for idx, p in enumerate(video_paths):\n",
    "        _, tail = os.path.split(p)\n",
    "        name = tail.split(\".\")[0]\n",
    "\n",
    "        np_path = output_dir + f\"/{name}.npz\"\n",
    "        output_filename = f\"{name}.npz\"\n",
    "        path = output_dir\n",
    "\n",
    "        if p not in saved_useless_files:\n",
    "            if np_path in output_files:\n",
    "                print(f\"{idx} => File already processed: \", np_path)\n",
    "            else:\n",
    "                try:\n",
    "                    detect_blinking = DetectBlinking(\n",
    "                        p, 0.3, 4,\n",
    "                        crop_face=True,\n",
    "                        return_features=True,\n",
    "                        process=True,\n",
    "                        logs=False,\n",
    "                    )\n",
    "                    # print(f\"Path: {p}\")\n",
    "                    video_features, ear_features = detect_blinking.process_video()\n",
    "\n",
    "                    if video_features is not None and len(video_features) > 0:\n",
    "                        video_features = np.array(video_features)\n",
    "                        ear_features = np.array(ear_features)\n",
    "                        reduced_features = apply_pca(video_features, n_components=50)\n",
    "                        final_features = np.concatenate((reduced_features, ear_features), axis=1)\n",
    "                        print(f\"{idx} Final Features: \", final_features.shape)\n",
    "                        save(path, output_filename, final_features)\n",
    "                    else:\n",
    "                        useless_files.append(p)\n",
    "                        print(f\"{idx} Video Skipped...\", len(useless_files))\n",
    "                        np.save(\"./out/useless_files.npy\", useless_files)\n",
    "                except Exception as e:\n",
    "                    print(f\"Found error is path: {p}\")\n",
    "                    print(f\"Error: {e}\")\n",
    "                    useless_files.append(p)\n",
    "                    print(f\"{idx} Video Skipped...\", len(useless_files))\n",
    "                    np.save(\"./out/useless_files.npy\", useless_files)\n",
    "\n",
    "        else:\n",
    "            print(f\"File found in useless list: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1735395466427,
     "user": {
      "displayName": "Faraz Khalil",
      "userId": "15571848968372978323"
     },
     "user_tz": 0
    },
    "id": "OWqKZL9u46HM",
    "outputId": "8a6eee42-3e25-49ef-8cd0-8c1bfe68240f"
   },
   "outputs": [],
   "source": [
    "extracted_fake_paths_npy = np.array(glob.glob(fake_output_dir + \"/*.npz\"))\n",
    "extracted_real_paths_npy = np.array(glob.glob(real_output_dir + \"/*.npz\"))\n",
    "\n",
    "print(\"extracted_fake_paths_npy: \", extracted_fake_paths_npy.shape)\n",
    "print(\"extracted_real_paths_npy: \", extracted_real_paths_npy.shape)\n",
    "\n",
    "fake_mp4_paths = glob.glob(fake_dataset_dir + \"/*/*/*/*.mp4\")\n",
    "real_mp4_paths = glob.glob(real_dataset_dir + \"/*/*/*/*.mp4\")\n",
    "print(\"fake_mp4_paths: \", len(fake_mp4_paths))\n",
    "print(\"real_mp4_paths: \", len(real_mp4_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and save them as .npy files\n",
    "\n",
    "extract_features_and_save(fake_mp4_paths, fake_output_dir, extracted_fake_paths_npy)\n",
    "extract_features_and_save(real_mp4_paths, real_output_dir, extracted_real_paths_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_paths = np.load(\"out/useless_files.npy\")\n",
    "print(len(useless_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MpwFkCE0h2XL"
   },
   "outputs": [],
   "source": [
    "def pad_to_max_length(array, max_length, pad_value = 0):\n",
    "    if array.ndim == 2:\n",
    "        padded = np.pad(array, ((0, max_length - len(array)), (0, 0)), mode=\"constant\", constant_values=pad_value)\n",
    "    else:\n",
    "        padded = np.pad(array, (0, max_length - len(array)), mode='constant', constant_values=pad_value)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_fake_paths_npy = np.array(glob.glob(fake_output_dir + \"/*.npz\"))\n",
    "extracted_real_paths_npy = np.array(glob.glob(real_output_dir + \"/*.npz\"))\n",
    "\n",
    "print(\"extracted_fake_paths_npy: \", len(extracted_fake_paths_npy), extracted_fake_paths_npy.shape)\n",
    "print(\"extracted_real_paths_npy: \", len(extracted_real_paths_npy), extracted_real_paths_npy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, path in enumerate(extracted_fake_paths_npy):\n",
    "    features = np.load(path)\n",
    "    try:\n",
    "        print(f\"{idx} --- {features[\"features\"][0][0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{idx} --- {path}\")\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features and pad them to the same length\n",
    "fake_features = []\n",
    "real_features = []\n",
    "\n",
    "# Load the features, \n",
    "for idx, path in enumerate(extracted_fake_paths_npy):\n",
    "    features = np.load(path)[\"features\"]\n",
    "    fake_features.append(features)\n",
    "\n",
    "for idx, path in enumerate(extracted_real_paths_npy):\n",
    "    features = np.load(path)[\"features\"]\n",
    "    real_features.append(features)\n",
    "\n",
    "# fake_features = np.array(fake_features)\n",
    "# real_features = np.array(real_features)\n",
    "\n",
    "print(\"fake_features: \", len(fake_features))\n",
    "print(\"real_features: \", len(real_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max length of the features\n",
    "max_length = max(max(len(features) for features in fake_features), max(len(features) for features in real_features))\n",
    "print(\"Max length of features: \", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max length of the features\n",
    "min_length = min(min(len(features) for features in fake_features), min(len(features) for features in real_features))\n",
    "print(\"Min length of features: \", min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the features\n",
    "fake_features_padded = []\n",
    "real_features_padded = []\n",
    "\n",
    "\n",
    "for idx, features in enumerate(fake_features):\n",
    "    if len(features) > 200:\n",
    "        padded_arr = pad_to_max_length(features, max_length)\n",
    "        fake_features_padded.append(padded_arr)\n",
    "\n",
    "fake_features_padded = np.array(fake_features_padded)\n",
    "\n",
    "for idx, features in enumerate(real_features):\n",
    "    if len(features) > 200:\n",
    "        padded_arr = pad_to_max_length(features, max_length)\n",
    "        real_features_padded.append(padded_arr)\n",
    "\n",
    "real_features_padded = np.array(real_features_padded)\n",
    "\n",
    "print(\"fake_features_padded: \", fake_features_padded.shape)\n",
    "print(\"real_features_padded: \", real_features_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"{relative_base_path}out/pca_features\", fake_features=fake_features_padded, real_features=real_features_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(\"./out/pca_features.npz\")\n",
    "fake_features = features[\"fake_features\"]\n",
    "real_features = features[\"real_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_features_padded = fake_features\n",
    "real_features_padded = real_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fake and real features in a single .npy file with the respective targets i.e. 0 for fake and 1 for real\n",
    "fake_targets = np.zeros(fake_features_padded.shape[0])\n",
    "real_targets = np.ones(real_features_padded.shape[0])\n",
    "\n",
    "all_features = np.concatenate((fake_features_padded, real_features_padded), axis=0)\n",
    "all_targets = np.concatenate((fake_targets, real_targets), axis=0)\n",
    "\n",
    "print(\"X_dataset: \", all_features.shape)\n",
    "print(\"Y_dataset: \", all_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"{relative_base_path}out/pca_features_dataset\", X_dataset=all_features, Y_dataset=all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
